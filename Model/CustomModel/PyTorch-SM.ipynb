{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> &uarr;   Ensure Kernel is set to  &uarr;  </div><br><div style=\"text-align: right\"> \n",
    "conda_python3  </div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Estimator Bring your own Script\n",
    "\n",
    "In this notebook we will go through and run a PyTorch model to classify the junctions as priority, signal and roundabout as seen in data prep.\n",
    "\n",
    "The outline of this notebook is \n",
    "\n",
    "1. to prepare a training script (provided).\n",
    "\n",
    "2. use the AWS provided PyTorch container and provide our script to it.\n",
    "\n",
    "3. Run training.\n",
    "\n",
    "4. deploy model to end point.\n",
    "\n",
    "5. Test using an image in couple of possible ways "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upgrade Sagemaker so we can access the latest containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sagemaker"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will import the libraries and set up the initial variables we will be using in this lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "ON_SAGEMAKER_NOTEBOOK = False\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "if ON_SAGEMAKER_NOTEBOOK:\n",
    "    role = sagemaker.get_execution_role()\n",
    "else:\n",
    "    role = \"arn:aws:iam::ACCOUNTNUMHERE:role/service-role/AmazonSageMaker-ExecutionRole\"\n",
    "\n",
    "import boto3\n",
    "client = boto3.client('sagemaker-runtime')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, replace **your-unique-bucket-name** with the name of bucket you created in the data-prep notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"labeled-images\"\n",
    "# key = \"data-folder\"   (in case you structure your data as your-bucket/data-folder) \n",
    "training_data_uri=\"s3://{}\".format(bucket)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Estimator\n",
    "\n",
    "Use AWS provided open source containers, these containers can be extended by starting with the image provided by AWS and the add additional installs in dockerfile\n",
    "\n",
    "or you can use requirements.txt in source_dir to install additional libraries.\n",
    "\n",
    "Below code is for PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PyTorch(entry_point='ptModelCode.py',\n",
    "                    role=role,\n",
    "                    framework_version='1.8',\n",
    "                    instance_count=1,\n",
    "                    instance_type='ml.p3.2xlarge',\n",
    "                    py_version='py3',\n",
    "                    # available hyperparameters: emsize, nhid, nlayers, lr, clip, epochs, batch_size,\n",
    "                    #                            bptt, dropout, tied, seed, log_interval\n",
    "                    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we call the estimators fit method with the URI location of the training data to start the training <br>\n",
    "**Note:** This cell takes approximately **20 mins** to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "estimator.fit(training_data_uri)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **NOTE:** <br>\n",
    "If at this point your kernel disconnects from the server (you can tell because the kernel in the top right hand corner will say **No Kernel**),<br>you can reattach to the training job (so you dont to start the training job again).<br>Follow the steps below\n",
    "1. Scoll your notebook to the top and set the kernel to the recommended kernel specified in the top right hand corner of the notebook\n",
    "2. Go to your SageMaker console, Go to Training Jobs and copy the name of the training job you were disconnected from\n",
    "3. Scoll to the bottom of this notebook, paste your training job name to replace the **your-training-job-name** in the cell\n",
    "4. Replace **your-unique-bucket-name** with the name of bucket you created in the data-prep notebook\n",
    "5. Run the edited cell\n",
    "6. Return to this cell and continue executing the rest of this notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call the model_data method on the estimator to find the location of the trained model artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.model_data\n",
    "latest_model = estimator.model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "latest_model = \"s3://sagemaker-us-west-2-ACCOUNTNUMBER/pytorch-training-2023-05-30-18-32-57-416/output/model.tar.gz\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploying a model\n",
    "Once trained, deploying a model is a simple call.\n",
    "\n",
    "**Note:** Replace the **'your_model_uri'** with the URI from the cell above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorchModel\n",
    "pytorch_model = PyTorchModel(model_data=latest_model, \n",
    "                             role=role, \n",
    "                             entry_point='ptInfCode.py', \n",
    "                             framework_version='1.8',\n",
    "                             py_version='py3')\n",
    "predictor = pytorch_model.deploy(instance_type='ml.m5.2xlarge', initial_instance_count=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets get the endpoint name from predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictor.endpoint_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our endpoint is up and running, lets test it with all of our unseen images and see how well it does\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print (\"Bucket:\",bucket)\n",
    "    s3_client = boto3.client('s3')\n",
    "    test_files=[]\n",
    "    response = s3_client.list_objects_v2(\n",
    "        Bucket=bucket,\n",
    "        Prefix='test'\n",
    "    )\n",
    "    for item in response['Contents']:\n",
    "        test_files.append(item['Key'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test manual model\n",
    "import io\n",
    "import json\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "s3 = boto3.resource('s3', region_name='us-west-2')\n",
    "s3_bucket = s3.Bucket(bucket)\n",
    "\n",
    "endpoint_name = predictor.endpoint_name\n",
    "\n",
    "# image category, fight probabily, no fight probability\n",
    "inference_data = []\n",
    "df = None\n",
    "for file_object in test_files:\n",
    "    #print(file_object)\n",
    "    object = s3_bucket.Object(file_object)\n",
    "\n",
    "    tmp = tempfile.NamedTemporaryFile()\n",
    "\n",
    "    with open(tmp.name, 'wb') as f:\n",
    "        object.download_fileobj(f)\n",
    "    \n",
    "    # whatever you need to do\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType='application/x-image',\n",
    "        Body=open(tmp.name, 'rb').read())\n",
    "    result = json.loads(response['Body'].read().decode(\"utf-8\"))\n",
    "    # file object name indicates if test image is a fight or no fight\n",
    "    # result \n",
    "    inf_data_row = [file_object,file_object.split('/')[1], result[0]['Fight'], result[0]['No Fight']]\n",
    "    inference_data.append(inf_data_row)\n",
    "\n",
    "df = pd.DataFrame(inference_data, columns=['File','Category','FProb','NoFProb'])\n",
    "\n",
    "# clean up inference instance\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n",
    "df['FProb'] = pd.to_numeric(df['FProb'], errors='coerce')\n",
    "df['NoFProb'] = pd.to_numeric(df['NoFProb'], errors='coerce')\n",
    "df.groupby('Category')['FProb'].plot(legend=True, figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.options.display.max_colwidth = 100\n",
    "df\n",
    "#show file names where fight was greater than 30%\n",
    "df.loc[df[\"FProb\"]>0.3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_THRESHHOLD = 0.213\n",
    "NEG_THRESHHOLD = 0.735\n",
    "\n",
    "# convert probabilities to float\n",
    "#df['FProb'] = pd.to_numeric(df['FProb'], errors='coerce')\n",
    "#df['NoFProb'] = pd.to_numeric(df['NoFProb'], errors='coerce')\n",
    "\n",
    "# separate frames into fight and no labeled photos\n",
    "fight_fl = df['Category']=='Fight'\n",
    "no_fight_fl = df['Category']=='NoFight'\n",
    "\n",
    "fight_df = df[fight_fl]\n",
    "no_fight_df = df[no_fight_fl]\n",
    "\n",
    "fight_detected_fl = fight_df['FProb']>=POS_THRESHHOLD\n",
    "fight_detected_df = fight_df[fight_detected_fl]\n",
    "\n",
    "no_fight_detected_fl = no_fight_df['NoFProb']>=NEG_THRESHHOLD\n",
    "no_fight_detected_df = no_fight_df[no_fight_detected_fl]\n",
    "\n",
    "true_positive = fight_detected_df['Category'].count()\n",
    "true_negative = no_fight_detected_df['Category'].count()\n",
    "false_positive = no_fight_df['Category'].count()-true_negative\n",
    "false_negative = fight_df['Category'].count()-true_positive\n",
    "\n",
    "print(\"Labeled fights:\", fight_df['Category'].count(), \"Labeled No Fights:\", no_fight_df['Category'].count())\n",
    "print(\"True Positive:\",true_positive,\"True Negative:\",true_negative, \"False Negative:\",false_negative, \"False Positive:\",false_positive)\n",
    "\n",
    "precision=true_positive/(true_positive+false_positive)\n",
    "recall=true_positive/(true_positive+false_negative)\n",
    "accuracy=(true_positive+true_negative)/(df['Category'].count())\n",
    "print(\"Precision:\",precision)\n",
    "print(\"Recall:\",recall)\n",
    "print(\"Accuracy:\",accuracy)\n",
    "print(\"F Score:\", 2*(precision*recall)/(precision+recall))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare this identical test set with Custom Labels inference to verify Custom Labels is in fact doing a much better job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom Labels\n",
    "import io\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "IMAGE_MODEL_ARN = \"arn:aws:rekognition:us-west-2:ACCOUNTNUMNBER:project/fight-detection-ratio-adjusted-1\"\n",
    "\n",
    "s3 = boto3.resource('s3', region_name='us-west-2')\n",
    "s3_bucket = s3.Bucket(bucket)\n",
    "s3_client = boto3.client('s3')\n",
    "model_client = boto3.client('rekognition')\n",
    "\n",
    "# image category, fight probabily, no fight probability\n",
    "inference_data = []\n",
    "# dek=0\n",
    "for file_object in test_files:\n",
    "    #print(file_object)\n",
    "    response = model_client.detect_custom_labels(Image={'S3Object': {'Bucket': bucket, 'Name': file_object}},\n",
    "                                                     MinConfidence=1,\n",
    "                                                     ProjectVersionArn=IMAGE_MODEL_ARN)\n",
    "    no_fight_prob = 0\n",
    "    fight_prob = 0\n",
    "    #print(response)\n",
    "    #{'CustomLabels': [{'Name': 'No Fight', 'Confidence': 98.4469985961914}, {'Name': 'Fight', 'Confidence': 1.5530000925064087}], 'ResponseMetadata': {'RequestId': 'c5382ac8-05e9-453a-802d-d74d0516bdc2', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'c5382ac8-05e9-453a-802d-d74d0516bdc2', 'content-type': 'application/x-amz-json-1.1', 'content-length': '117', 'date': 'Thu, 13 Apr 2023 20:53:57 GMT'}, 'RetryAttempts': 0}}\n",
    "    # First element will be the class probabilities\n",
    "    # classes found is on or more dictionary objects with class name and confidence\n",
    "    for classes_found in response['CustomLabels']:\n",
    "        #print(classes_found)\n",
    "        # Don't know which classes will be returned\n",
    "        if classes_found[\"Name\"] == \"No Fight\":\n",
    "            no_fight_prob = classes_found[\"Confidence\"]/100\n",
    "        if classes_found[\"Name\"] == \"Fight\":\n",
    "            fight_prob = classes_found[\"Confidence\"]/100\n",
    "      \n",
    "    #print (file_object, no_fight_prob, fight_prob)\n",
    "#     dek+=1\n",
    "#     if dek == 15:\n",
    "#         break\n",
    "   #result = json.loads(response['Body'].read().decode(\"utf-8\"))\n",
    "    # file object name indicates if test image is a fight or no fight\n",
    "    # result \n",
    "    inf_data_row = [file_object,file_object.split('/')[1], fight_prob, no_fight_prob]\n",
    "    inference_data.append(inf_data_row)\n",
    "cl_df = pd.DataFrame(inference_data, columns=['File','Category','FProb','NoFProb'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_THRESHHOLD = 0.213\n",
    "NEG_THRESHHOLD = 0.735\n",
    "\n",
    "# convert probabilities to float\n",
    "cl_df['FProb'] = pd.to_numeric(cl_df['FProb'], errors='coerce')\n",
    "cl_df['NoFProb'] = pd.to_numeric(cl_df['NoFProb'], errors='coerce')\n",
    "\n",
    "# separate frames into fight and no labeled photos\n",
    "cl_fight_fl = cl_df['Category']=='Fight'\n",
    "cl_no_fight_fl = cl_df['Category']=='NoFight'\n",
    "\n",
    "cl_fight_df = cl_df[cl_fight_fl]\n",
    "cl_no_fight_df = cl_df[cl_no_fight_fl]\n",
    "\n",
    "cl_fight_detected_fl = cl_fight_df['FProb']>=POS_THRESHHOLD\n",
    "cl_fight_detected_df = cl_fight_df[cl_fight_detected_fl]\n",
    "\n",
    "cl_no_fight_detected_fl = cl_no_fight_df['NoFProb']>=NEG_THRESHHOLD\n",
    "cl_no_fight_detected_df = cl_no_fight_df[cl_no_fight_detected_fl]\n",
    "\n",
    "cl_true_positive = cl_fight_detected_df['Category'].count()\n",
    "cl_true_negative = cl_no_fight_detected_df['Category'].count()\n",
    "cl_false_positive = cl_no_fight_df['Category'].count()-cl_true_negative\n",
    "cl_false_negative = cl_fight_df['Category'].count()-cl_true_positive\n",
    "\n",
    "print(\"Labeled fights:\", cl_fight_df['Category'].count(), \"Labeled No Fights:\", cl_no_fight_df['Category'].count())\n",
    "print(\"True Positive:\",cl_true_positive,\"True Negative:\",cl_true_negative, \"False Negative:\",cl_false_negative, \"False Positive:\",cl_false_positive)\n",
    "\n",
    "cl_precision=cl_true_positive/(cl_true_positive+cl_false_positive)\n",
    "cl_recall=cl_true_positive/(cl_true_positive+cl_false_negative)\n",
    "cl_accuracy=(cl_true_positive+cl_true_negative)/(cl_df['Category'].count())\n",
    "print(\"CL Precision:\",cl_precision)\n",
    "print(\"CL Recall:\",cl_recall)\n",
    "print(\"CL Accuracy:\",cl_accuracy)\n",
    "print(\"CL F Score:\", 2*(cl_precision*cl_recall)/(cl_precision+cl_recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_df.to_csv('custom_lables.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us view the JSON response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_df.groupby('Category')['FProb'].plot(legend=True, figsize=(10,10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "#fight_df\n",
    "no_fight_detected_df.count()\n",
    "#fight_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up\n",
    "\n",
    "When we're done with the endpoint, we can just delete it and the backing instances will be released.  Run the following cell to delete the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attach to a training job that has been left to run \n",
    "\n",
    "If your kernel becomes disconnected and your training has already started, you can reattach to the training job.<br>\n",
    "In the cell below, replace **your-unique-bucket-name** with the name of bucket you created in the data-prep notebook<br>\n",
    "Simply look up the training job name and replace the **your-training-job-name** and then run the cell below. <br>\n",
    "Once the training job is finished, you can continue the cells after the training cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "bucket = \"your-unique-bucket-name\"\n",
    "\n",
    "training_job_name = 'your-training-job-name'\n",
    "\n",
    "if 'your-training' not in training_job_name:\n",
    "    estimator = sagemaker.estimator.Estimator.attach(training_job_name=training_job_name, sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
